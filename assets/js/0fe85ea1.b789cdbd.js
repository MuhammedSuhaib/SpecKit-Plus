"use strict";(globalThis.webpackChunkfrontend=globalThis.webpackChunkfrontend||[]).push([[38],{2842:e=>{e.exports=JSON.parse('{"version":{"pluginId":"default","version":"current","label":"Next","banner":null,"badge":false,"noIndex":false,"className":"docs-version-current","isLast":true,"docsSidebars":{"tutorialSidebar":[{"type":"category","label":"Robotic Nervous System","items":[{"type":"link","href":"/dockathon/docs/module-01-robotic-nervous-system/intro","label":"From Digital AI to Physical AI","docId":"module-01-robotic-nervous-system/intro","unlisted":false}],"collapsed":true,"collapsible":true},{"type":"category","label":"Digital Twin","items":[{"type":"link","href":"/dockathon/docs/module-02-digital-twin/intro","label":"Why we simulate","docId":"module-02-digital-twin/intro","unlisted":false}],"collapsed":true,"collapsible":true},{"type":"category","label":"AI Robot Brain","items":[{"type":"link","href":"/dockathon/docs/module-03-ai-robot-brain/intro","label":"Module 3: AI Robot Brain - Perception Overview","docId":"module-03-ai-robot-brain/intro","unlisted":false}],"collapsed":true,"collapsible":true},{"type":"category","label":"Vision Language Action","items":[{"type":"link","href":"/dockathon/docs/module-04-vision-language-action/intro","label":"Module 4: Vision-Language-Action (VLA) - Overview","docId":"module-04-vision-language-action/intro","unlisted":false}],"collapsed":true,"collapsible":true}]},"docs":{"module-01-robotic-nervous-system/chapter-01-ros2-basics":{"id":"module-01-robotic-nervous-system/chapter-01-ros2-basics","title":"Nodes, Topics, Services","description":"ROS 2 (Robot Operating System 2) provides the communication framework for robotic applications. Understanding its core concepts of nodes, topics, and services is essential for building distributed robotic systems."},"module-01-robotic-nervous-system/chapter-02-hardware":{"id":"module-01-robotic-nervous-system/chapter-02-hardware","title":"Setting up Jetson Orin and RealSense","description":"The Jetson Orin and Intel RealSense cameras form a powerful combination for robotic applications, providing high-performance computing and accurate depth perception capabilities."},"module-01-robotic-nervous-system/intro":{"id":"module-01-robotic-nervous-system/intro","title":"From Digital AI to Physical AI","description":"The transition from digital artificial intelligence to physical embodied intelligence represents a paradigm shift in how we conceptualize and implement AI systems. While traditional AI operates in virtual spaces, embodied intelligence brings cognition into the physical world through robots and other physical agents that can perceive, reason, and act in real environments.","sidebar":"tutorialSidebar"},"module-02-digital-twin/chapter-03-gazebo-urdf":{"id":"module-02-digital-twin/chapter-03-gazebo-urdf","title":"Building a robot body in XML (URDF)","description":"Unified Robot Description Format (URDF) is an XML-based format used to describe robotic systems in ROS. It defines the physical and visual properties of a robot, including links, joints, and their relationships."},"module-02-digital-twin/chapter-04-isaac-sim":{"id":"module-02-digital-twin/chapter-04-isaac-sim","title":"Intro to NVIDIA Omniverse & USD","description":"NVIDIA Isaac Sim, built on the Omniverse platform, provides a powerful simulation environment for robotics development. It uses Universal Scene Description (USD) as its core data format, enabling high-fidelity physics simulation and photorealistic rendering."},"module-02-digital-twin/intro":{"id":"module-02-digital-twin/intro","title":"Why we simulate","description":"Simulation serves as the cornerstone of embodied intelligence development, allowing us to test and validate robotic systems in safe, controlled virtual environments before deploying them in the real world. Through simulation, we can accelerate development cycles, reduce costs, and ensure safety by identifying potential issues before physical implementation.","sidebar":"tutorialSidebar"},"module-03-ai-robot-brain/chapter-05-computer-vision":{"id":"module-03-ai-robot-brain/chapter-05-computer-vision","title":"Chapter 05: Computer Vision - YOLO and Depth Cameras","description":"Introduction to Computer Vision in Robotics"},"module-03-ai-robot-brain/chapter-06-navigation":{"id":"module-03-ai-robot-brain/chapter-06-navigation","title":"Chapter 06: Navigation - Nav2 and SLAM","description":"Introduction to Robot Navigation"},"module-03-ai-robot-brain/intro":{"id":"module-03-ai-robot-brain/intro","title":"Module 3: AI Robot Brain - Perception Overview","description":"Introduction to Perception in AI Robotics","sidebar":"tutorialSidebar"},"module-04-vision-language-action/capstone-logic":{"id":"module-04-vision-language-action/capstone-logic","title":"Capstone Logic: Complete VLA System Implementation","description":"The capstone logic represents the integration and orchestration of all components in a Vision-Language-Action (VLA) system. This section covers the end-to-end implementation and deployment considerations for complete VLA applications."},"module-04-vision-language-action/chapter-07-whisper":{"id":"module-04-vision-language-action/chapter-07-whisper","title":"Chapter 7: Whisper Voice Integration - Voice-to-Action Logic","description":"This chapter explores the integration of OpenAI Whisper for speech recognition combined with ROS 2 for robotic control, creating a voice-to-action pipeline that enables natural human-robot interaction."},"module-04-vision-language-action/chapter-08-vla-models":{"id":"module-04-vision-language-action/chapter-08-vla-models","title":"chapter-08-vla-models","description":""},"module-04-vision-language-action/intro":{"id":"module-04-vision-language-action/intro","title":"Module 4: Vision-Language-Action (VLA) - Overview","description":"Vision-Language-Action (VLA) systems represent the convergence of perception, cognition, and action in embodied AI. This module explores how visual processing, language understanding, and robotic control can be integrated to create intelligent agents capable of interacting naturally with their environment.","sidebar":"tutorialSidebar"},"module-04-vision-language-action/README":{"id":"module-04-vision-language-action/README","title":"Module 4: Vision-Language-Action (VLA) Models","description":"This directory contains the complete documentation for Module 4"},"module-04-vision-language-action/vla-models":{"id":"module-04-vision-language-action/vla-models","title":"VLA Models (Vision-Language-Action)","description":"Vision-Language-Action (VLA) models are multimodal neural networks that jointly process visual observations, natural language instructions, and produce robotic actions. These models represent a breakthrough in embodied AI, enabling robots to interact with the world through a combination of perception, language understanding, and action execution."},"module-04-vision-language-action/whisper-integration":{"id":"module-04-vision-language-action/whisper-integration","title":"Whisper Voice Integration","description":"Whisper is an open-source automatic speech recognition (ASR) system developed by OpenAI. It is designed to handle multiple languages and various speech recognition tasks, making it ideal for voice integration in embodied intelligence systems."}}}}')}}]);