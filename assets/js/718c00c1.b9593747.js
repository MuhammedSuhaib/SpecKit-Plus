"use strict";(globalThis.webpackChunkfrontend=globalThis.webpackChunkfrontend||[]).push([[372],{6105:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>a,default:()=>u,frontMatter:()=>r,metadata:()=>o,toc:()=>c});const o=JSON.parse('{"id":"module-04-vision-language-action/intro","title":"Module 4: Vision-Language-Action (VLA) - Overview","description":"Vision-Language-Action (VLA) systems represent the convergence of perception, cognition, and action in embodied AI. This module explores how visual processing, language understanding, and robotic control can be integrated to create intelligent agents capable of interacting naturally with their environment.","source":"@site/docs/module-04-vision-language-action/intro.md","sourceDirName":"module-04-vision-language-action","slug":"/module-04-vision-language-action/intro","permalink":"/dockathon/docs/module-04-vision-language-action/intro","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/module-04-vision-language-action/intro.md","tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"sidebar_position":1},"sidebar":"tutorialSidebar","previous":{"title":"Module 3: AI Robot Brain - Perception Overview","permalink":"/dockathon/docs/module-03-ai-robot-brain/intro"}}');var t=i(2714),s=i(8885);const r={sidebar_position:1},a="Module 4: Vision-Language-Action (VLA) - Overview",l={},c=[{value:"Introduction",id:"introduction",level:2},{value:"Core Components",id:"core-components",level:2},{value:"Learning Path",id:"learning-path",level:2},{value:"Module Structure",id:"module-structure",level:2},{value:"Prerequisites",id:"prerequisites",level:2}];function d(e){const n={a:"a",h1:"h1",h2:"h2",header:"header",li:"li",ol:"ol",p:"p",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"module-4-vision-language-action-vla---overview",children:"Module 4: Vision-Language-Action (VLA) - Overview"})}),"\n",(0,t.jsx)(n.p,{children:"Vision-Language-Action (VLA) systems represent the convergence of perception, cognition, and action in embodied AI. This module explores how visual processing, language understanding, and robotic control can be integrated to create intelligent agents capable of interacting naturally with their environment."}),"\n",(0,t.jsx)(n.h2,{id:"introduction",children:"Introduction"}),"\n",(0,t.jsx)(n.p,{children:"Vision-Language-Action (VLA) systems are at the forefront of embodied intelligence, enabling robots to:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Perceive their environment through visual sensors"}),"\n",(0,t.jsx)(n.li,{children:"Understand natural language commands and instructions"}),"\n",(0,t.jsx)(n.li,{children:"Execute appropriate physical actions in response"}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"This integration represents a paradigm shift from traditional robotic systems that process these modalities separately, toward unified systems that can handle complex, real-world tasks through natural human-robot interaction."}),"\n",(0,t.jsx)(n.h2,{id:"core-components",children:"Core Components"}),"\n",(0,t.jsx)(n.p,{children:"The VLA framework consists of three essential components:"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Vision Processing"}),": Understanding the visual environment through cameras and other sensors"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Language Understanding"}),": Interpreting natural language commands and contextual information"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Action Execution"}),": Controlling robotic systems to perform physical tasks"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"learning-path",children:"Learning Path"}),"\n",(0,t.jsx)(n.p,{children:"In this module, we'll explore:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Voice-to-Action systems using OpenAI Whisper and ROS 2"}),"\n",(0,t.jsx)(n.li,{children:"Integration of Large Language Models with robotic systems"}),"\n",(0,t.jsx)(n.li,{children:"Complete capstone project implementing an autonomous humanoid system"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"module-structure",children:"Module Structure"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.a,{href:"/dockathon/docs/module-04-vision-language-action/chapter-07-whisper",children:"Chapter 7: Whisper Voice Integration"})," - Voice-to-Action logic with OpenAI Whisper and ROS 2"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.a,{href:"/dockathon/docs/module-04-vision-language-action/chapter-08-vla-models",children:"Chapter 8: VLA Models"})," - Integrating LLMs with Robotics"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.a,{href:"./capstone.md",children:"Capstone: Autonomous Humanoid Project"})," - Final integration of all components"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,t.jsx)(n.p,{children:"Before starting this module, ensure you have:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Basic understanding of ROS 2 concepts"}),"\n",(0,t.jsx)(n.li,{children:"Familiarity with neural networks and deep learning"}),"\n",(0,t.jsx)(n.li,{children:"Understanding of computer vision fundamentals"}),"\n"]})]})}function u(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}},8885:(e,n,i)=>{i.d(n,{R:()=>r,x:()=>a});var o=i(9378);const t={},s=o.createContext(t);function r(e){const n=o.useContext(s);return o.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:r(e.components),o.createElement(s.Provider,{value:n},e.children)}}}]);