"use strict";(globalThis.webpackChunkfrontend=globalThis.webpackChunkfrontend||[]).push([[451],{4309:(e,n,t)=>{t.d(n,{A:()=>a});var i=t(9378),o=t(2714);function a({opacity:e=.12,fullScreen:n=!0}){const t=(0,i.useRef)(null);(0,i.useEffect)(()=>{const i=t.current,o=i.getContext("2d");(()=>{const t=i.parentElement;let a,r;if(n)a=window.innerWidth,r=window.innerHeight;else{const e=t.getBoundingClientRect();a=e.width,r=e.height}i.width=a,i.height=r;const s=Math.floor(a/14)+1,c=Array(s).fill(0),l=()=>{o.fillStyle=`rgba(0,0,0,${e})`,o.fillRect(0,0,a,r),o.fillStyle="#00FF41",o.font="14px monospace";for(let e=0;e<c.length;e++){const n=String.fromCharCode(12448+96*Math.random());o.fillText(n,14*e,14*c[e]),14*c[e]>r&&Math.random()>.975&&(c[e]=0),c[e]++}};l();const d=setInterval(l,45);if(n){const e=()=>{i.width=window.innerWidth,i.height=window.innerHeight};return window.addEventListener("resize",e),()=>{clearInterval(d),window.removeEventListener("resize",e)}}})()},[e,n]);const a=n?{position:"fixed",inset:0,zIndex:0,pointerEvents:"none"}:{position:"absolute",top:0,left:0,width:"100%",height:"100%",zIndex:0,pointerEvents:"none"};return(0,o.jsx)("canvas",{ref:t,style:a})}},4982:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>c,default:()=>h,frontMatter:()=>s,metadata:()=>i,toc:()=>d});const i=JSON.parse('{"id":"module-04-vision-language-action/intro","title":"intro","description":"Vision-Language-Action (VLA) - Overview","source":"@site/docs/module-04-vision-language-action/intro.mdx","sourceDirName":"module-04-vision-language-action","slug":"/module-04-vision-language-action/intro","permalink":"/dockathon/docs/module-04-vision-language-action/intro","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/module-04-vision-language-action/intro.mdx","tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"sidebar_position":1},"sidebar":"tutorialSidebar","previous":{"title":"Chapter 06: Navigation - Nav2 and SLAM","permalink":"/dockathon/docs/module-03-ai-robot-brain/chapter-06-navigation"},"next":{"title":"Chapter 7: Whisper Voice Integration - Voice-to-Action Logic","permalink":"/dockathon/docs/module-04-vision-language-action/chapter-07-whisper"}}');var o=t(2714),a=t(8885),r=t(4309);const s={sidebar_position:1},c="CHAPTER 4",l={},d=[{value:"Vision-Language-Action (VLA) - Overview",id:"vision-language-action-vla---overview",level:2}];function u(e){const n={h1:"h1",h2:"h2",header:"header",p:"p",...(0,a.R)(),...e.components};return(0,o.jsxs)("div",{style:{position:"relative",overflow:"hidden",minHeight:"300px",marginBottom:"2rem"},children:[(0,o.jsx)(r.A,{}),(0,o.jsxs)("div",{style:{position:"relative",zIndex:1,color:"var(--ifm-color-emphasis-900)",padding:"2rem"},children:[(0,o.jsx)(n.header,{children:(0,o.jsx)(n.h1,{id:"chapter-4",children:"CHAPTER 4"})}),(0,o.jsx)(n.h2,{id:"vision-language-action-vla---overview",children:"Vision-Language-Action (VLA) - Overview"}),(0,o.jsx)(n.p,{children:"Vision-Language-Action (VLA) systems represent the convergence of perception, cognition, and action in embodied AI. This module explores how visual processing, language understanding, and robotic control can be integrated to create intelligent agents capable of interacting naturally with their environment."})]})]})}function h(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(u,{...e})}):u(e)}},8885:(e,n,t)=>{t.d(n,{R:()=>r,x:()=>s});var i=t(9378);const o={},a=i.createContext(o);function r(e){const n=i.useContext(a);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function s(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:r(e.components),i.createElement(a.Provider,{value:n},e.children)}}}]);