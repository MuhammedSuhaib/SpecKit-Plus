---
sidebar_position: 1
---

# Module 4: Vision-Language-Action (VLA) Models

This section covers Vision-Language-Action models in Embodied Intelligence, including voice integration with Whisper, VLA model architectures, and capstone implementation logic.

## Overview

Vision-Language-Action (VLA) models represent a significant advancement in embodied intelligence, enabling robots and AI agents to perceive their environment, understand natural language commands, and execute physical actions. This module explores the integration of these three modalities to create intelligent agents capable of complex interactions with the physical world.

## Topics Covered

1. [Whisper Voice Integration](./whisper-integration.md) - Integrating speech recognition and processing capabilities
2. [VLA Models](./vla-models.md) - Understanding Vision-Language-Action model architectures
3. [Capstone Logic](./capstone-logic.md) - Implementing end-to-end VLA system logic

## Learning Objectives

By the end of this module, you should understand:
- How to integrate voice recognition systems with robotic platforms
- The fundamentals of Vision-Language-Action model architectures
- How to implement complete VLA systems for embodied intelligence applications